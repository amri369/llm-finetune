{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-04T16:03:17.768990Z",
     "start_time": "2025-10-04T16:03:13.337505Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T16:03:19.163631Z",
     "start_time": "2025-10-04T16:03:19.160712Z"
    }
   },
   "cell_type": "code",
   "source": "instruct_tune_dataset",
   "id": "397b04be887ac2a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 56167\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 6807\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T16:03:19.840327Z",
     "start_time": "2025-10-04T16:03:19.836995Z"
    }
   },
   "cell_type": "code",
   "source": "instruct_tune_dataset[\"train\"][0]",
   "id": "8ffebf8a806692df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction\\nQuestion: Nancy and Rose are making bracelets, and there are eight beads in each bracelet. Nancy has 40 metal beads and 20 more pearl beads. Rose has 20 crystal beads and twice as many stone beads as crystal beads. How many bracelets can Nancy and Rose make?\\nAnswer: Nancy has 40 + 20 = 60 pearl beads. So, Nancy has a total of 40 + 60 = 100 beads. Rose has 2 x 20 = 40 stone beads. So, Rose has 20 + 40 = 60 beads. Thus, Nancy and Rose have 100 + 60 = 160 beads altogether. Therefore, they can make 160 / 8 = 20 bracelets. The answer is 20.\\n[Question]Ms. Estrella is an entrepreneur with a startup company having 10 employees. The company makes a revenue of $400000 a month, paying 10% in taxes, 5% of the remaining amount on marketing and ads, 20% of the remaining amount on operational costs, and 15% of the remaining amount on employee wages. Assuming each employee receives the same wage, calculate the amount of money each employee is paid monthly.\\n[Answer]The company pays a total of 10 / 100 * $400000 = $40000 on taxes. After taxes, the company revenue is $400000 - $40000 = $360,000. The costs of marketing and ads campaign are 5 / 100 * $360000 = $18000. After deducting the costs of marketing and adds campaign, the company remains with = $342,000 in revenue. Operational costs for the company are 20 / 100 * $342000 = $68400. After taking out the operational costs, the company remains with $342000 - $68400 = $273600. The company also pays employee wages of 15 / 100 * $273600 = $41040. If the total number of employees is 10, each employee is paid a salary of $41040 / 10 = $4104 per month. The answer is 4104.\\nQ: Princeton had a gender reveal party and invited all of his fellow employees to celebrate with him. If the total number of guests were 60, and 2/3 were male guests, how many female guests attended the party?\\nA: The number of males in the party was 2 males / 3 guests * 60 guests = 40 males. If the total number of people at the party was 60, then there were 60 guests - 40 males = 20 females. The answer is 20.\\nQuestion: Joshua bought 25 oranges for $12.50. If he sells each one for 60c, how much profit in cents will he make on each orange?\\nAnswer: $1 is equivalent to 100 cents so $12.50 is equivalent to 100 * 12.50 = 1250 cents. He bought 25 oranges for 1250 cents so each orange cost 1250 / 25 = 50 cents each. If he sells each orange for 60 cents, he is making a profit of 60 - 50 = 10 cents on each one. The answer is 10.\\n[Question]Wilson decides to go sledding on some nearby hills. On the 2 tall hills, he sleds down them 4 times each and on the 3 small hills, he sled down them half as often as he sleds down the tall hills. How many times did he sled down the hills?\\n[Answer]On the tall hills, he sleds down 2 tall hills * 4 times per tall hill = 8 times. He sleds down the small hills half as often so he sleds down each hill 4 times per tall hill / 2 = 2 times per small hill. On the small hills, he sleds down 3 small hills * 2 times per small hill = 6 times. So in total, Wilson sled down the hills 8 times on tall hills + 6 times on small hills = 14 times. The answer is 14.\\n[Question]Natalia is riding a bicycle for the cycling competition. On Monday she rode 40 kilometers and on Tuesday 50 kilometers. On Wednesday she rode 50% fewer kilometers than the day before. On Thursday she rode as many as the sum of the kilometers from Monday and Wednesday. How many kilometers did Natalie ride in total?\\n[Answer]\\n### Response\\n',\n",
       " 'response': 'On Wednesday she covered half of the distance from Tuesday, so 50 / 2 = 25 kilometers. On Thursday she rode 40 + 25 = 65 kilometers. In total, Natalia rode 40 + 50 + 25 + 65 = 180 kilometers. The answer is 180.',\n",
       " 'source': 'cot_gsm8k'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T16:03:20.543730Z",
     "start_time": "2025-10-04T16:03:20.537350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "instruct_tune_dataset"
   ],
   "id": "1e0a363294648eb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 34333\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 4771\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T16:03:21.185012Z",
     "start_time": "2025-10-04T16:03:21.179909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000))\n",
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"train\"].select(range(200))\n",
    "instruct_tune_dataset"
   ],
   "id": "9ca31977042efda3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T16:03:21.827476Z",
     "start_time": "2025-10-04T16:03:21.825260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "def create_prompt(sample: Dict[str, str]) -> str:\n",
    "    prompt = sample[\"prompt\"]\n",
    "    response = sample[\"response\"]\n",
    "    bos_token, eos_token = \"<s>\", \"</s>\"\n",
    "    original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "    prompt = prompt.replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "    \n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += \"### Instruction:\"\n",
    "    full_prompt += \"\\n\" + system_message\n",
    "    full_prompt += \"\\n\\n### Input:\"\n",
    "    full_prompt += \"\\n\" + response\n",
    "    full_prompt += \"\\n\\n### Response:\"\n",
    "    full_prompt += \"\\n\" + prompt\n",
    "    full_prompt += eos_token\n",
    "    \n",
    "    return full_prompt\n",
    "    "
   ],
   "id": "ff33c44e1b65393f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T16:03:22.421246Z",
     "start_time": "2025-10-04T16:03:22.418032Z"
    }
   },
   "cell_type": "code",
   "source": "create_prompt(instruct_tune_dataset[\"train\"][0])",
   "id": "eb4e9d2395f7573b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\\n\\n### Response:\\nWhat are different types of grass?</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:25:15.990848Z",
     "start_time": "2025-10-05T06:25:15.632066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map='cpu',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "id": "fcfbf02d5ea9a980",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      6\u001B[39m nf4_config = BitsAndBytesConfig(\n\u001B[32m      7\u001B[39m    load_in_4bit=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m      8\u001B[39m    bnb_4bit_quant_type=\u001B[33m\"\u001B[39m\u001B[33mnf4\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      9\u001B[39m    bnb_4bit_use_double_quant=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     10\u001B[39m    bnb_4bit_compute_dtype=torch.bfloat16\n\u001B[32m     11\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmistralai/Mistral-7B-Instruct-v0.1\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcpu\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnf4_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m     18\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m tokenizer = AutoTokenizer.from_pretrained(\u001B[33m\"\u001B[39m\u001B[33mmistralai/Mistral-7B-v0.1\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     22\u001B[39m tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/code/llm-finetune/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    602\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    603\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    606\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    608\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    609\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    610\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/code/llm-finetune/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001B[39m, in \u001B[36mrestore_default_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    275\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    276\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    278\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    279\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/code/llm-finetune/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4884\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   4875\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m transformers_explicit_filename.endswith(\n\u001B[32m   4876\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m.safetensors\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4877\u001B[39m     ) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m transformers_explicit_filename.endswith(\u001B[33m\"\u001B[39m\u001B[33m.safetensors.index.json\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m   4878\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   4879\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4880\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4881\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtransformers_explicit_filename\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   4882\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m4884\u001B[39m hf_quantizer, config, dtype, device_map = \u001B[43mget_hf_quantizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_agent\u001B[49m\n\u001B[32m   4886\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4888\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   4889\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   4890\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4891\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/code/llm-finetune/.venv/lib/python3.12/site-packages/transformers/quantizers/auto.py:319\u001B[39m, in \u001B[36mget_hf_quantizer\u001B[39m\u001B[34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001B[39m\n\u001B[32m    316\u001B[39m     hf_quantizer = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m     \u001B[43mhf_quantizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    320\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    321\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    322\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    323\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    324\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    325\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    326\u001B[39m     dtype = hf_quantizer.update_dtype(dtype)\n\u001B[32m    327\u001B[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/code/llm-finetune/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:98\u001B[39m, in \u001B[36mBnb4BitHfQuantizer.validate_environment\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m version.parse(importlib.metadata.version(\u001B[33m\"\u001B[39m\u001B[33mbitsandbytes\u001B[39m\u001B[33m\"\u001B[39m)) < version.parse(\u001B[33m\"\u001B[39m\u001B[33m0.43.1\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m     97\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch.cuda.is_available():\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m     99\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    100\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    101\u001B[39m         )\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mintegrations\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m validate_bnb_backend_availability\n\u001B[32m    104\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001B[31mImportError\u001B[39m: The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1."
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:24:23.361339Z",
     "start_time": "2025-10-05T06:24:23.249543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"mistral_instruct_generation\",\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=5,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    learning_rate=1e-4,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    dataset_text_field=\"text\",\n",
    ")"
   ],
   "id": "6fdfc5160f0aa397",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:24:54.888965Z",
     "start_time": "2025-10-05T06:24:54.682142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=0.6,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "id": "af469520007e8571",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T06:24:55.906184Z",
     "start_time": "2025-10-05T06:24:55.761391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  processing_class=tokenizer,\n",
    "  args=args,\n",
    "  train_dataset=instruct_tune_dataset[\"train\"],\n",
    "  eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ],
   "id": "b7672142d130eb17",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtrl\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SFTTrainer\n\u001B[32m      4\u001B[39m trainer = SFTTrainer(\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m   model=\u001B[43mmodel\u001B[49m,\n\u001B[32m      6\u001B[39m   peft_config=peft_config,\n\u001B[32m      7\u001B[39m   processing_class=tokenizer,\n\u001B[32m      8\u001B[39m   args=args,\n\u001B[32m      9\u001B[39m   train_dataset=instruct_tune_dataset[\u001B[33m\"\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     10\u001B[39m   eval_dataset=instruct_tune_dataset[\u001B[33m\"\u001B[39m\u001B[33mtest\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     11\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f84764f050281af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
